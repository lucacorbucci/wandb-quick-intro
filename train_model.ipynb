{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model and log the results on Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how to log the results of a model training on Weights & Biases. \n",
    "The code used to train the model is taken from the [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from utils import log_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# These two additional parameters are for logging to wandb \n",
    "# the images and the corresponding model predictions. \n",
    "NUM_BATCHES_TO_LOG = 10\n",
    "NUM_IMAGES_PER_BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data \n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model we will use\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of a W&B Run object \n",
    "\n",
    "The run object is the object used to log data to W&B.\n",
    "We can create a run object using the `wandb.init()` function.\n",
    "There are several parameters that can be passed to the `wandb.init()` function to customize the run object:\n",
    "- `project`: The name of the project to which the experiment belongs.\n",
    "- `name`: The name of the experiment.\n",
    "- `config`: A dictionary of configuration parameters for the run we're starting. These are static parameters that usually do not change during the process and that will be logged to WANDB. They can be useful to identify the run and to compare different runs.\n",
    "- `tags`: A list of tags to add to the run. These can be useful to filter the different experiments and to group them by tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucacorbucci\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lcorbucci/wandb-quick-intro/wandb/run-20240220_085534-3faqwh0w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucacorbucci/wandb-quick-intro/runs/3faqwh0w' target=\"_blank\">twinkling-orchid-2</a></strong> to <a href='https://wandb.ai/lucacorbucci/wandb-quick-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucacorbucci/wandb-quick-intro' target=\"_blank\">https://wandb.ai/lucacorbucci/wandb-quick-intro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucacorbucci/wandb-quick-intro/runs/3faqwh0w' target=\"_blank\">https://wandb.ai/lucacorbucci/wandb-quick-intro/runs/3faqwh0w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(project=\"wandb-quick-intro\", # name of the project in which we want to store our runs\n",
    "                        config={\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"num_classes\": num_classes,\n",
    "                            \"criterion\": \"CrossEntropyLoss\", \n",
    "                            \"optimizer\": \"Adam\"                   \n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and log the results\n",
    "\n",
    "We can use the classic functions to train models with Pytorch and add just a few lines of code to \n",
    "log the results to wandb:\n",
    "- In the train function we call the log function to log the loss of the model after each step. In this case we use i*epoch as the step number that we want to use as \"x\" axis in the plot.\n",
    "- Then we call the log function to log the accuracy of the model on the test set after each epoch.\n",
    "- We also log a table with some of the images of the test dataset with the corresponding vector of confidence of the model.\n",
    "\n",
    "In the end log on Wandb the final model that we trained. To log it we have to save the model on disk, then we create an Artifact object with the model file and log it to Wandb using the function log_artifact(). \n",
    "We can do the same thing with the dataset used to train the model so that the experiment will be reproducible in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1325\n",
      "Epoch [1/5], Step [200/600], Loss: 0.0916\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1102\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1518\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0731\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0565\n",
      "Test Accuracy of the model on the 10000 test images: 98.25 %\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0622\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0125\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0147\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0434\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0256\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0238\n",
      "Test Accuracy of the model on the 10000 test images: 98.82 %\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0166\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0308\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0625\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0049\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0590\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0165\n",
      "Test Accuracy of the model on the 10000 test images: 99.03 %\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0021\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0060\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0588\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0511\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0949\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0018\n",
      "Test Accuracy of the model on the 10000 test images: 99.08 %\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0308\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0051\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0170\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0239\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0055\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0168\n",
      "Test Accuracy of the model on the 10000 test images: 99.04 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 15.4%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▆███</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▃▂▂▁▂▁▂▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▁▂▂▂▃▃▄▄▁▂▃▃▄▅▅▆▁▂▃▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>99.04</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.01683</td></tr><tr><td>step</td><td>2396</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twinkling-orchid-2</strong> at: <a href='https://wandb.ai/lucacorbucci/wandb-quick-intro/runs/3faqwh0w' target=\"_blank\">https://wandb.ai/lucacorbucci/wandb-quick-intro/runs/3faqwh0w</a><br/>Synced 5 W&B file(s), 5 media file(s), 1606 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240220_085534-3faqwh0w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        wandb.log({\"loss\": loss, \"step\": i*epoch})\n",
    "        \n",
    "    # ✨ W&B: Create a Table to store predictions for each test step\n",
    "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "    for digit in range(10):\n",
    "        columns.append(\"score_\" + str(digit))\n",
    "    test_table = wandb.Table(columns=columns)\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    log_counter = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if log_counter < NUM_BATCHES_TO_LOG:\n",
    "                log_test_predictions(images, labels, outputs, predicted, test_table, log_counter, NUM_IMAGES_PER_BATCH)\n",
    "                log_counter += 1\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        wandb.log({\"epoch\" : epoch, \"acc\" : accuracy})\n",
    "\n",
    "    wandb.log({\"test_predictions\" : test_table})\n",
    "\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "# We store the model on wandb\n",
    "artifact = wandb.Artifact(name=\"model\", type=\"model\")\n",
    "artifact.add_file(local_path=\"model.ckpt\")\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
