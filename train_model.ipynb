{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model and log the results on Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how to log the results of a model training on Weights & Biases. \n",
    "The code used to train the model is taken from the [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from utils import log_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# These two additional parameters are for logging to wandb \n",
    "# the images and the corresponding model predictions. \n",
    "NUM_BATCHES_TO_LOG = 10\n",
    "NUM_IMAGES_PER_BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data \n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model we will use\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of a W&B Run object \n",
    "\n",
    "The run object is the object used to log data to W&B.\n",
    "We can create a run object using the `wandb.init()` function.\n",
    "There are several parameters that can be passed to the `wandb.init()` function to customize the run object:\n",
    "- `project`: The name of the project to which the experiment belongs.\n",
    "- `name`: The name of the experiment.\n",
    "- `config`: A dictionary of configuration parameters for the run we're starting. These are static parameters that usually do not change during the process and that will be logged to WANDB. They can be useful to identify the run and to compare different runs.\n",
    "- `tags`: A list of tags to add to the run. These can be useful to filter the different experiments and to group them by tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = wandb.init(project=\"Temperatures\", # name of the project in which we want to store our runs\n",
    "                        config={\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"num_classes\": num_classes,\n",
    "                            \"criterion\": \"CrossEntropyLoss\", \n",
    "                            \"optimizer\": \"Adam\"                   \n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and log the results\n",
    "\n",
    "We can use the classic functions to train models with Pytorch and add just a few lines of code to \n",
    "log the results to wandb:\n",
    "- In the train function we call the log function to log the loss of the model after each step. In this case we use i*epoch as the step number that we want to use as \"x\" axis in the plot.\n",
    "- Then we call the log function to log the accuracy of the model on the test set after each epoch.\n",
    "- We also log a table with some of the images of the test dataset with the corresponding vector of confidence of the model.\n",
    "\n",
    "In the end log on Wandb the final model that we trained. To log it we have to save the model on disk, then we create an Artifact object with the model file and log it to Wandb using the function log_artifact(). \n",
    "We can do the same thing with the dataset used to train the model so that the experiment will be reproducible in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        wandb.log({\"loss\": loss, \"step\": i*epoch})\n",
    "        \n",
    "    # âœ¨ W&B: Create a Table to store predictions for each test step\n",
    "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "    for digit in range(10):\n",
    "        columns.append(\"score_\" + str(digit))\n",
    "    test_table = wandb.Table(columns=columns)\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    log_counter = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if log_counter < NUM_BATCHES_TO_LOG:\n",
    "                log_test_predictions(images, labels, outputs, predicted, test_table, log_counter, NUM_IMAGES_PER_BATCH)\n",
    "                log_counter += 1\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        wandb.log({\"epoch\" : epoch, \"acc\" : accuracy})\n",
    "\n",
    "    wandb.log({\"test_predictions\" : test_table})\n",
    "\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "# We store the model on wandb\n",
    "artifact = wandb.Artifact(name=\"model\", type=\"model\")\n",
    "artifact.add_file(local_path=\"model.ckpt\")\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
